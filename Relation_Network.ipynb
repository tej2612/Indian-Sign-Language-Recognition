{"cells":[{"cell_type":"markdown","source":["# Relation Network Implementation on Indian Sign Language Classification\n","\n","### Ayush Muralidharan: PES1UG22AM912\n","### Tejas V Bhat: PES1UG22AM909\n","### Atharv Revankar: PES1UG22AM920\n","### Prarthana Kini: PES1UG22AM119"],"metadata":{"id":"WL8BiJB-zlYk"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"saucJWWdzF5A","outputId":"ebf9b069-278d-4281-acb6-e186d8e986db"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"827hZJ048lxN"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader\n","import random\n","import os\n","from PIL import Image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kWtVnbEE_KkW","outputId":"d549fef7-6c86-485f-a715-da7ba68f9ba7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n"]},{"cell_type":"markdown","source":["### Embedding Network\n","A pre-trained ResNet-18 is used as the feature extractor.\n","The final classification layer is replaced with an identity layer, outputting raw feature embeddings."],"metadata":{"id":"zRzQgJP0zhOu"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wWE7aKjb_MC8","outputId":"d67e655a-dcbd-437b-c600-7832fb757621"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["class EmbeddingNet(nn.Module):\n","    def __init__(self):\n","        super(EmbeddingNet, self).__init__()\n","        self.model = models.resnet18(pretrained=True)\n","        self.model.fc = nn.Identity()  # Remove the classification layer\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","embedding_net = EmbeddingNet().to(device)\n"]},{"cell_type":"markdown","source":["### Relation Network\n","A fully connected network that computes relation scores between support and query features.\n","- Input size: concatenated feature dimensions of the query and support embeddings.\n","- Output: a similarity score (relation score) using a sigmoid activation.\n","\n","\n","\n"],"metadata":{"id":"pr9kvkdfzu_S"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_j3zM7G_N0x"},"outputs":[],"source":["class RelationNetwork(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(RelationNetwork, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = torch.sigmoid(self.fc2(x))\n","        return x\n","\n","relation_net = RelationNetwork(input_size=1024, hidden_size=256).to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4vFiym8x_RDf","outputId":"75f9b7ff-f543-417b-f467-e26407a72e9a"},"outputs":[{"data":{"text/plain":["RelationNetwork(\n","  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n","  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",")"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["\n","relation_net"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAr-LwYA_T73"},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomVerticalFlip(p=0.5),\n","    transforms.RandomRotation(30),\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])"]},{"cell_type":"markdown","source":["### CustomDataset Class:\n","Organizes data from class folders and applies data augmentation (e.g., resizing, flipping, rotation, color jitter, and normalization)."],"metadata":{"id":"wciRa4y50NU8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQphiHSc_Y3q"},"outputs":[],"source":["class CustomDataset:\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.classes = os.listdir(root_dir)\n","        self.data = []\n","        for class_name in self.classes:\n","            class_dir = os.path.join(root_dir, class_name)\n","            for img_file in os.listdir(class_dir):\n","                self.data.append((os.path.join(class_dir, img_file), class_name))\n","        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.classes)}\n","\n","    def __getitem__(self, idx):\n","        img_path, label = self.data[idx]\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, self.class_to_idx[label]\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","source":["train_dataset = CustomDataset(\"/Users/ayushmuralidharan/Desktop/AOML/AOML PROJECT/MAML-Pytorch-master/Experiment 2/Dataset_INDIAN_FULL/train\", transform=transform)"],"metadata":{"id":"KES9NjMa0c2M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Few-Shot Sampling\n","- Episodes are dynamically generated using N_WAY, K_SHOT, and Q_QUERY sampling strategies.\n","- Support and query sets are created for each episode."],"metadata":{"id":"hcWEUybx0g6q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLDS8Vfj_bzK"},"outputs":[],"source":["def sample_episode_n_shot(dataset, n_way, k_shot, q_query):\n","    classes = random.sample(dataset.classes, n_way)\n","    support_images, support_labels, query_images, query_labels = [], [], [], []\n","\n","    for label, class_name in enumerate(classes):\n","        class_samples = [img for img, cls in dataset.data if cls == class_name]\n","        support_sample = random.sample(class_samples, k_shot)\n","        query_sample = random.sample([s for s in class_samples if s not in support_sample], q_query)\n","\n","        support_images.extend(support_sample)\n","        support_labels.extend([label] * k_shot)\n","        query_images.extend(query_sample)\n","        query_labels.extend([label] * q_query)\n","\n","    return support_images, support_labels, query_images, query_labels\n","\n","def prepare_episode(support_images, query_images, transform, device):\n","    def load_images(image_paths):\n","        return [transform(Image.open(img).convert(\"RGB\")).unsqueeze(0).to(device) for img in image_paths]\n","\n","    support_tensors = load_images(support_images)\n","    query_tensors = load_images(query_images)\n","    return support_tensors, query_tensors\n"]},{"cell_type":"markdown","source":["### Training Loop\n","Each episode calculates relation scores between query and support embeddings.\n","The MSE loss between predicted relation scores and target labels is minimized.\n","Hyperparameters:\n","- N_WAY: Number of classes in an episode.\n","- K_SHOT: Number of support images per class.\n","- Q_QUERY: Number of query images per class."],"metadata":{"id":"ilnSMUXq0wUi"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"TW2aPNaf_ldj","outputId":"fff72bc8-481c-4a60-fc2b-f018941bcc74"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["Episode [10/500], Loss: 3.7513\n","Episode [20/500], Loss: 3.7519\n","Episode [30/500], Loss: 3.7514\n","Episode [40/500], Loss: 3.7506\n","Episode [50/500], Loss: 3.7501\n","Episode [60/500], Loss: 3.7501\n","Episode [70/500], Loss: 3.7501\n","Episode [80/500], Loss: 3.7501\n","Episode [90/500], Loss: 3.7501\n","Episode [100/500], Loss: 3.7501\n","Episode [110/500], Loss: 3.7501\n","Episode [120/500], Loss: 3.7501\n","Episode [130/500], Loss: 3.7500\n","Episode [140/500], Loss: 3.7501\n","Episode [150/500], Loss: 3.7501\n","Episode [160/500], Loss: 3.7500\n","Episode [170/500], Loss: 3.7501\n","Episode [180/500], Loss: 3.7501\n","Episode [190/500], Loss: 3.7501\n","Episode [200/500], Loss: 3.7500\n","Episode [210/500], Loss: 3.7500\n","Episode [220/500], Loss: 3.7500\n","Episode [230/500], Loss: 3.7500\n","Episode [240/500], Loss: 3.7501\n","Episode [250/500], Loss: 3.7500\n","Episode [260/500], Loss: 3.7500\n","Episode [270/500], Loss: 3.7500\n","Episode [280/500], Loss: 3.7500\n","Episode [290/500], Loss: 3.7500\n","Episode [300/500], Loss: 3.7501\n","Episode [310/500], Loss: 3.7500\n","Episode [320/500], Loss: 3.7501\n","Episode [330/500], Loss: 3.7501\n","Episode [340/500], Loss: 3.7500\n","Episode [350/500], Loss: 3.7501\n","Episode [360/500], Loss: 3.7501\n","Episode [370/500], Loss: 3.7501\n","Episode [380/500], Loss: 3.7500\n","Episode [390/500], Loss: 3.7501\n","Episode [400/500], Loss: 3.7501\n","Episode [410/500], Loss: 3.7500\n","Episode [420/500], Loss: 3.7500\n","Episode [430/500], Loss: 3.7500\n","Episode [440/500], Loss: 3.7500\n","Episode [450/500], Loss: 3.7500\n","Episode [460/500], Loss: 3.7501\n","Episode [470/500], Loss: 3.7500\n","Episode [480/500], Loss: 3.7500\n","Episode [490/500], Loss: 3.7501\n","Episode [500/500], Loss: 3.7501\n"]}],"source":["num_episodes = 500\n","n_way = 4\n","k_shot = 5\n","q_query = 5\n","\n","# Loss and optimizer\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(list(embedding_net.parameters()) + list(relation_net.parameters()), lr=1e-4)\n","\n","# Training loop\n","for episode in range(num_episodes):\n","    # Sample episode\n","    support_images, support_labels, query_images, query_labels = sample_episode_n_shot(\n","        train_dataset, n_way, k_shot, q_query\n","    )\n","\n","    # Prepare tensors\n","    support_tensors, query_tensors = prepare_episode(\n","        support_images, query_images, transform, device\n","    )\n","\n","    # Extract support features\n","    support_features = [embedding_net(img).squeeze(0) for img in support_tensors]\n","\n","    # Initialize loss\n","    loss = 0\n","\n","    # Process each query image\n","    for i, query_tensor in enumerate(query_tensors):\n","        query_feature = embedding_net(query_tensor).squeeze(0)\n","\n","        # Compute relation scores\n","        relation_scores = []\n","        for support_feature in support_features:\n","            combined_feature = torch.cat((support_feature, query_feature), dim=0)\n","            score = relation_net(combined_feature.unsqueeze(0))\n","            relation_scores.append(score)\n","\n","        # Reshape and compute loss\n","        predicted_scores = torch.cat(relation_scores)\n","        target = torch.zeros(n_way).to(device)\n","        target[query_labels[i]] = 1\n","\n","        loss += criterion(predicted_scores, target)\n","\n","    # Backpropagate and optimize\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Print progress\n","    if (episode + 1) % 10 == 0:\n","        print(f\"Episode [{episode + 1}/{num_episodes}], Loss: {loss.item():.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JooVMPApOjRm"},"outputs":[],"source":["def prepare_episode_for_accuracy(support_images, query_images):\n","    def load_and_stack(image_paths):\n","        # Load images, apply transformations, and stack them into a batch\n","        tensors = [transform(Image.open(img).convert(\"RGB\")).to(device) for img in image_paths]\n","        return torch.stack(tensors)  # Shape: [batch_size, 3, 224, 224]\n","\n","    # Process support and query sets\n","    support_tensors = load_and_stack(support_images)  # Shape: [n_support, 3, 224, 224]\n","    query_tensors = load_and_stack(query_images)      # Shape: [n_query, 3, 224, 224]\n","\n","    return support_tensors, query_tensors\n"]},{"cell_type":"markdown","source":["### Evaluation\n","- Class prototypes (mean embeddings for each class) are computed from the support set.\n","- Query embeddings are compared to prototypes using the Relation Network.\n","- Accuracy: Percentage of correctly predicted query labels.\n","python\n","\n"],"metadata":{"id":"nL2Nkvi71Eyb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKmyujjiN4sG"},"outputs":[],"source":["def calculate_accuracy(relation_net, embedding_net, support_tensors, support_labels, query_tensors, query_labels):\n","    \"\"\"\n","    Calculate the accuracy of the Relation Network on the query set.\n","\n","    Args:\n","        relation_net: The Relation Network model.\n","        embedding_net: The Embedding Network model.\n","        support_tensors: Batched tensor for support set images.\n","        support_labels: Labels corresponding to the support set.\n","        query_tensors: Batched tensor for query set images.\n","        query_labels: Labels corresponding to the query set.\n","\n","    Returns:\n","        Accuracy (in percentage).\n","    \"\"\"\n","    # Set models to evaluation mode\n","    relation_net.eval()\n","    embedding_net.eval()\n","\n","    correct = 0\n","    total = len(query_labels)\n","    n_way = len(set(support_labels))  # Number of classes in the support set\n","\n","    # Compute class prototypes (mean feature vectors for each class in the support set)\n","    class_prototypes = []\n","    for class_idx in range(n_way):\n","        # Get features for all support images belonging to the current class\n","        class_indices = [i for i, label in enumerate(support_labels) if label == class_idx]\n","        class_support_features = embedding_net(support_tensors[class_indices])\n","        class_prototype = class_support_features.mean(dim=0)  # Compute mean feature vector\n","        class_prototypes.append(class_prototype)\n","\n","    class_prototypes = torch.stack(class_prototypes).to(device)  # Shape: [n_way, feature_dim]\n","\n","    # Predict labels for query set\n","    for i, query_tensor in enumerate(query_tensors):\n","        query_feature = embedding_net(query_tensor.unsqueeze(0))  # Shape: [1, feature_dim]\n","\n","        # Compute relation scores between query and all class prototypes\n","        relation_scores = [\n","            relation_net(torch.cat((prototype, query_feature.squeeze(0)), dim=-1).unsqueeze(0))\n","            for prototype in class_prototypes\n","        ]\n","\n","        # Determine the predicted class\n","        predicted_class = torch.argmax(torch.cat(relation_scores).view(1, n_way), dim=1).item()\n","        print(f\"Predicted:{predicted_class} Actual:{query_labels[i]}\")\n","        # Check if the prediction is correct\n","        if predicted_class == query_labels[i]:\n","            correct += 1\n","\n","\n","    # Calculate accuracy\n","    accuracy = (correct / total) * 100\n","    print(total)\n","    print(correct)\n","    print(f\"Accuracy: {accuracy:.2f}%\")\n","    return accuracy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKRdjg-jUDbs"},"outputs":[],"source":["test_dataset = CustomDataset(\"/Users/ayushmuralidharan/Desktop/AOML/AOML PROJECT/MAML-Pytorch-master/Experiment 2/Dataset_INDIAN_FULL/test\", transform=transform)\n","\n","# Sample an episode from the test dataset\n","eval_n_way = 3\n","eval_k_shot = 5\n","eval_q_query = 5\n","\n","# Generate support and query sets\n","support_images, support_labels, query_images, query_labels = sample_episode_n_shot(\n","    test_dataset, eval_n_way, eval_k_shot, eval_q_query\n",")\n","\n","# Load tensors for support and query sets\n","support_tensors, query_tensors = prepare_episode_for_accuracy(support_images, query_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jPPmPgvCPv59","outputId":"12a1e7c1-0cf7-4893-a39b-e94229a0c421"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted:0 Actual:0\n","Predicted:0 Actual:0\n","Predicted:1 Actual:0\n","Predicted:2 Actual:0\n","Predicted:2 Actual:0\n","Predicted:1 Actual:1\n","Predicted:1 Actual:1\n","Predicted:2 Actual:1\n","Predicted:2 Actual:1\n","Predicted:2 Actual:1\n","Predicted:2 Actual:2\n","Predicted:2 Actual:2\n","Predicted:1 Actual:2\n","Predicted:1 Actual:2\n","Predicted:1 Actual:2\n","15\n","6\n","Accuracy: 40.00%\n","Final Evaluation Accuracy: 40.00%\n"]}],"source":["# Prepare support and query tensors\n","support_tensors, query_tensors = prepare_episode_for_accuracy(support_images, query_images)\n","\n","# Calculate accuracy\n","accuracy = calculate_accuracy(\n","    relation_net=relation_net,\n","    embedding_net=embedding_net,\n","    support_tensors=support_tensors,\n","    support_labels=support_labels,\n","    query_tensors=query_tensors,\n","    query_labels=query_labels,\n",")\n","\n","print(f\"Final Evaluation Accuracy: {accuracy:.2f}%\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":0}