{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrgB3ZE1gu9L"
      },
      "source": [
        "Matching Network Implementation on Indian Sign Language Classification\n",
        "\n",
        "### Ayush Muralidharan: PES1UG22AM912\n",
        "### Tejas V Bhat: PES1UG22AM909\n",
        "### Atharv Revankar: PES1UG22AM920\n",
        "### Prarthana Kini:Â PES1UG22AM119"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XowZjp28gu9P"
      },
      "source": [
        "First, we import all necessary libraries. We're using PyTorch for deep learning, torchvision for image processing, and PIL for image handling. These are essential tools for our sign language recognition system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHjJ6AOIgu9P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvwbCVcPgu9Q"
      },
      "source": [
        "Here we're setting up our device configuration. The code checks if a GPU is available - if yes, it uses CUDA for faster processing; otherwise, it falls back to CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY0hOKxAgu9Q",
        "outputId": "8ed4752e-02d3-4c14-f4f3-e6ca0d41164b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OHGaNOKgu9R"
      },
      "source": [
        "This is our custom dataset class for Indian Sign Language. It:\n",
        "\n",
        "-Loads images from our directory structure\n",
        "\n",
        "-Organizes them by class (different signs)\n",
        "\n",
        "-Handles image transformations\n",
        "\n",
        "-Currently manages 20 different sign classes ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upEZzSD-gu9R"
      },
      "outputs": [],
      "source": [
        "class SignLanguageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.classes = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
        "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        for class_name in self.classes:\n",
        "            class_dir = os.path.join(root_dir, class_name)\n",
        "            for user_dir in os.listdir(class_dir):\n",
        "                user_path = os.path.join(class_dir, user_dir)\n",
        "                if os.path.isdir(user_path):\n",
        "                    for img_name in os.listdir(user_path):\n",
        "                        if img_name.endswith('.jpg'):\n",
        "                            self.image_paths.append(os.path.join(user_path, img_name))\n",
        "                            self.labels.append(self.class_to_idx[class_name])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2PGNQSngu9S"
      },
      "source": [
        "For preprocessing our images, we:\n",
        "\n",
        "-Resize all images to 224x224 pixels\n",
        "\n",
        "-Convert them to tensors\n",
        "\n",
        "-Normalize them using ImageNet statistics\n",
        "\n",
        "This ensures consistent input to our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfodNeXNgu9S",
        "outputId": "d87dffab-ad9e-469b-a8db-a2315d2ca15b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 20\n",
            "Classes: ['afraid', 'agree', 'assistance', 'bad', 'become', 'college', 'doctor', 'from', 'pain', 'pray', 'secondary', 'skin', 'small', 'specific', 'stand', 'today', 'warn', 'which', 'work', 'you']\n"
          ]
        }
      ],
      "source": [
        "# Define the transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                       std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create dataset instance\n",
        "dataset_path = \"./Indian-sign-Language-Real-life-Words\"\n",
        "dataset = SignLanguageDataset(root_dir=dataset_path, transform=transform)\n",
        "\n",
        "# Basic information about the dataset\n",
        "print(f\"Number of classes: {len(dataset.classes)}\")\n",
        "print(\"Classes:\", dataset.classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayiToq2ygu9S"
      },
      "source": [
        "We implement few-shot learning by:\n",
        "\n",
        "Splitting our 20 classes into 16 training and 4 testing classes\n",
        "\n",
        "Using random selection with a fixed seed for reproducibility\n",
        "\n",
        "This tests the model's ability to learn new signs with limited data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-pLQYjMgu9S",
        "outputId": "dbab51d9-6878-43ab-cdca-d67efd889af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Meta-learning split:\n",
            "Meta-training classes (16): ['assistance', 'bad', 'become', 'college', 'doctor', 'from', 'pain', 'pray', 'secondary', 'skin', 'small', 'specific', 'stand', 'warn', 'work', 'you']\n",
            "Meta-testing classes (4): ['afraid' 'which' 'today' 'agree']\n"
          ]
        }
      ],
      "source": [
        "# Split classes into meta-train and meta-test\n",
        "np.random.seed(42)  # for reproducibility\n",
        "n_classes = len(dataset.classes)\n",
        "n_meta_test_classes = 4  # 20% of classes\n",
        "meta_test_classes = np.random.choice(dataset.classes, n_meta_test_classes, replace=False)\n",
        "meta_train_classes = [c for c in dataset.classes if c not in meta_test_classes]\n",
        "\n",
        "print(\"\\nMeta-learning split:\")\n",
        "print(f\"Meta-training classes ({len(meta_train_classes)}): {meta_train_classes}\")\n",
        "print(f\"Meta-testing classes ({len(meta_test_classes)}): {meta_test_classes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVw1N9KGgu9T"
      },
      "outputs": [],
      "source": [
        "class EmbeddingNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EmbeddingNetwork, self).__init__()\n",
        "        resnet = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "class MatchingNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MatchingNetwork, self).__init__()\n",
        "        self.embedding_network = EmbeddingNetwork()\n",
        "\n",
        "    def forward(self, support_images, support_labels, query_images):\n",
        "        support_embeddings = self.embedding_network(support_images)\n",
        "        query_embeddings = self.embedding_network(query_images)\n",
        "\n",
        "        similarities = self.cosine_similarity(query_embeddings, support_embeddings)\n",
        "        attention = torch.softmax(similarities, dim=1)\n",
        "\n",
        "        predicted_logits = torch.matmul(attention, torch.eye(len(support_labels.unique())).to(device)[support_labels])\n",
        "        return predicted_logits\n",
        "\n",
        "    def cosine_similarity(self, query, support):\n",
        "        query_norm = torch.norm(query, dim=1, keepdim=True)\n",
        "        support_norm = torch.norm(support, dim=1, keepdim=True)\n",
        "\n",
        "        query_normalized = query / query_norm\n",
        "        support_normalized = support / support_norm\n",
        "\n",
        "        similarities = torch.matmul(query_normalized, support_normalized.t())\n",
        "        return similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT4k3F_7gu9T"
      },
      "source": [
        "Our model architecture consists of:\n",
        "\n",
        "-An embedding network using ResNet18 for feature extraction\n",
        "\n",
        "-A matching network that compares query images with support images\n",
        "\n",
        "-Cosine similarity to measure how close images are to each other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRzBYGU5gu9T"
      },
      "outputs": [],
      "source": [
        "def create_episode(dataset, classes, n_way=5, n_support=5, n_query=5):\n",
        "    episode_classes = np.random.choice(classes, n_way, replace=False)\n",
        "\n",
        "    support_images = []\n",
        "    support_labels = []\n",
        "    query_images = []\n",
        "    query_labels = []\n",
        "\n",
        "    for label, class_name in enumerate(episode_classes):\n",
        "        class_indices = [i for i, l in enumerate(dataset.labels)\n",
        "                        if dataset.classes[l] == class_name]\n",
        "\n",
        "        selected_indices = np.random.choice(class_indices,\n",
        "                                          n_support + n_query,\n",
        "                                          replace=False)\n",
        "\n",
        "        support_idx = selected_indices[:n_support]\n",
        "        query_idx = selected_indices[n_support:n_support + n_query]\n",
        "\n",
        "        for idx in support_idx:\n",
        "            img, _ = dataset[idx]\n",
        "            support_images.append(img)\n",
        "            support_labels.append(label)\n",
        "\n",
        "        for idx in query_idx:\n",
        "            img, _ = dataset[idx]\n",
        "            query_images.append(img)\n",
        "            query_labels.append(label)\n",
        "\n",
        "    support_images = torch.stack(support_images).to(device)\n",
        "    support_labels = torch.tensor(support_labels).to(device)\n",
        "    query_images = torch.stack(query_images).to(device)\n",
        "    query_labels = torch.tensor(query_labels).to(device)\n",
        "\n",
        "    return support_images, support_labels, query_images, query_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcnQWw7hgu9T"
      },
      "source": [
        "For training, we:\n",
        "\n",
        "Create episodes with n-way, k-shot learning\n",
        "\n",
        "Each episode has support (training) and query (testing) images\n",
        "\n",
        "Use 5 support and 5 query images per class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZgJ1JkVgu9T"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "n_way = 4  # changed from 5 to 4-way classification\n",
        "n_support = 5  # 5-shot\n",
        "n_query = 5\n",
        "n_episodes = 1000\n",
        "\n",
        "# Initialize model, optimizer and loss function\n",
        "model = MatchingNetwork().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B5ZoZONgu9U"
      },
      "source": [
        "Our training configuration uses:\n",
        "\n",
        "4-way classification (4 classes at a time)\n",
        "\n",
        "5-shot learning (5 examples per class)\n",
        "\n",
        "1000 training episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVOwLnYDgu9U",
        "outputId": "19200945-8141-4e2b-a95f-9775984a200b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Atharv\\AppData\\Local\\Temp\\ipykernel_25116\\4032626248.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('matching_network.pth'))\n"
          ]
        }
      ],
      "source": [
        "def train_episode():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    support_images, support_labels, query_images, query_labels = create_episode(\n",
        "        dataset, classes=meta_train_classes, n_way=n_way, n_support=n_support, n_query=n_query\n",
        "    )\n",
        "\n",
        "    predicted_logits = model(support_images, support_labels, query_images)\n",
        "    loss = criterion(predicted_logits, query_labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, predicted = torch.max(predicted_logits, 1)\n",
        "    accuracy = (predicted == query_labels).float().mean()\n",
        "\n",
        "    return loss.item(), accuracy.item()\n",
        "\n",
        "# Training and model saving\n",
        "if not os.path.exists('matching_network.pth'):\n",
        "    print(\"Starting training...\")\n",
        "    for episode in range(n_episodes):\n",
        "        loss, accuracy = train_episode()\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            print(f\"Episode {episode + 1}/{n_episodes}\")\n",
        "            print(f\"Loss: {loss:.4f}\")\n",
        "            print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), 'matching_network.pth')\n",
        "else:\n",
        "    model.load_state_dict(torch.load('matching_network.pth'))\n",
        "    model.eval()\n",
        "\n",
        "def evaluate(n_test_episodes=100):\n",
        "    model.eval()\n",
        "    total_accuracy = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for episode in range(n_test_episodes):\n",
        "            support_images, support_labels, query_images, query_labels = create_episode(\n",
        "                dataset,\n",
        "                classes=meta_test_classes,\n",
        "                n_way=2,  # Use 4-way for testing\n",
        "                n_support=5,\n",
        "                n_query=5\n",
        "            )\n",
        "\n",
        "            predicted_logits = model(support_images, support_labels, query_images)\n",
        "            _, predicted = torch.max(predicted_logits, 1)\n",
        "            accuracy = (predicted == query_labels).float().mean()\n",
        "            total_accuracy += accuracy.item()\n",
        "\n",
        "    return total_accuracy / n_test_episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw7spabMgu9U"
      },
      "source": [
        "The training process:\n",
        "\n",
        "Trains the model episode by episode\n",
        "\n",
        "Uses Adam optimizer and CrossEntropy loss\n",
        "\n",
        "Saves the best model\n",
        "\n",
        "Achieves 71.64% accuracy on test classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJlc_hmsgu9U"
      },
      "source": [
        "Finally, we have a prediction function that:\n",
        "\n",
        "Takes a single image input\n",
        "\n",
        "Compares it with support examples\n",
        "\n",
        "Predicts the sign class\n",
        "\n",
        "Can work with completely new signs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWBbYZ8ygu9U",
        "outputId": "839c6a29-c611-44e0-dee1-da91ec3a4d4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating model...\n",
            "Test Accuracy over 500 episodes: 0.7164\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "print(\"\\nEvaluating model...\")\n",
        "test_accuracy = evaluate(n_test_episodes=500)\n",
        "print(f\"Test Accuracy over {500} episodes: {test_accuracy:.4f}\")\n",
        "\n",
        "def predict_single_image(image_path, support_size=5):\n",
        "    test_image = Image.open(image_path).convert('RGB')\n",
        "    test_image = transform(test_image).unsqueeze(0).to(device)\n",
        "\n",
        "    support_images = []\n",
        "    support_labels = []\n",
        "\n",
        "    for label, class_name in enumerate(meta_test_classes[:4]):  # Only use 4 classes\n",
        "        class_indices = [i for i, l in enumerate(dataset.labels)\n",
        "                        if dataset.classes[l] == class_name]\n",
        "        selected_indices = np.random.choice(class_indices, support_size, replace=False)\n",
        "\n",
        "        for idx in selected_indices:\n",
        "            img, _ = dataset[idx]\n",
        "            support_images.append(img)\n",
        "            support_labels.append(label)\n",
        "\n",
        "    support_images = torch.stack(support_images).to(device)\n",
        "    support_labels = torch.tensor(support_labels).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predicted_logits = model(support_images, support_labels, test_image)\n",
        "        _, predicted = torch.max(predicted_logits, 1)\n",
        "        predicted_class = meta_test_classes[predicted.item()]\n",
        "\n",
        "    return predicted_class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP70c01Pgu9V"
      },
      "source": [
        "This notebook demonstrates a practical application of few-shot learning for sign language recognition, achieving good accuracy even on previously unseen signs."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}